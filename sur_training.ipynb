{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5518e213-d162-4a54-834e-be2de24967f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from deap import base, creator, tools\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', True)\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functions import *\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Setting GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_CNN2().to(device)\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "#load trained model\n",
    "model.load_state_dict(torch.load('new_CNN_notebook.pth'))\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# divide the dataset into multiple sets\n",
    "batch_size = 256\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "subset = torch.utils.data.Subset(trainset, range(0, 40000))\n",
    "trainloader = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_set = torch.utils.data.Subset(trainset, range(40000, 50000))\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "partition_size = 2000\n",
    "num_partitions = int(len(subset) / partition_size)\n",
    "small_loaders = SmallLoaders(subset, partition_size, batch_size)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "classes = trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0009f19b-b5eb-4faf-ac88-b784ade82b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_processes = 8\n",
    "nBits = 30\n",
    "Chrom_length = model.fc2.weight.size()[0] * model.fc2.weight.size()[1] + model.fc2.bias.size()[0]\n",
    "w1 = -1.814       \n",
    "w2 = -0.6544\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(w1, w2))  # Minimize both objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, nBits*(Chrom_length))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", calcFitness, trainloader=trainloader, model=model, nBits=nBits, Chrom_length=Chrom_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ae4c1e-1046-4870-b132-3773b816a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating invalid population (5000) ...\n",
      "  Eval time:  9312.953893899918  seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a random population and find their real loss values to train the surrogate model with\n",
    "val1 = 4800\n",
    "\n",
    "popa = toolbox.population(n=5000)\n",
    "pop = []\n",
    "for ind in popa:\n",
    "    sep=separatevariables(ind, nBits, Chrom_length)\n",
    "    weightlist=[]\n",
    "    for weight in sep:\n",
    "        weightlist+=real2chrom(weight, nBits)\n",
    "    pop.append(creator.Individual(weightlist))\n",
    "\n",
    "# Evaluate invalid population\n",
    "invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "print(\"Evaluating invalid population (%i) ...\" % len(invalid_ind))\n",
    "\n",
    "eval_time = time.time()\n",
    "with ProcessPoolExecutor(max_processes) as executor:\n",
    "    fitnesses = list(executor.map(toolbox.evaluate, invalid_ind))\n",
    "print(\"  Eval time: \", time.time() - eval_time, \" seconds\")\n",
    "\n",
    "for ind, fit in zip(invalid_ind, fitnesses):\n",
    "    ind.fitness.values = fit\n",
    "\n",
    "X_train = []\n",
    "for individual in pop[:val1]:\n",
    "    weights_biases=separatevariables(individual, nBits, Chrom_length)\n",
    "    X_train.append(weights_biases)\n",
    "\n",
    "y_train = []\n",
    "for ind in pop[:val1]:\n",
    "    fitness = ind.fitness.values[0]\n",
    "    y_train.append(fitness)\n",
    "\n",
    "X_val = []\n",
    "for individual in pop[val1:]:\n",
    "    weights_biases=separatevariables(individual, nBits, Chrom_length)\n",
    "    X_val.append(weights_biases)\n",
    "    \n",
    "y_val = []\n",
    "for ind in pop[val1:]:\n",
    "    fitness = ind.fitness.values[0]\n",
    "    y_val.append(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbcad63-b666-4430-9455-2b859593a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 305.45379638671875\n",
      "Epoch 2/100, Loss: 164.28680419921875\n",
      "Epoch 3/100, Loss: 152.6299285888672\n",
      "Epoch 4/100, Loss: 52.191497802734375\n",
      "Epoch 5/100, Loss: 56.938758850097656\n",
      "Epoch 6/100, Loss: 80.64348602294922\n",
      "Epoch 7/100, Loss: 49.6628532409668\n",
      "Epoch 8/100, Loss: 15.127768516540527\n",
      "Epoch 9/100, Loss: 23.75521469116211\n",
      "Epoch 10/100, Loss: 43.83098602294922\n",
      "Epoch 11/100, Loss: 34.591678619384766\n",
      "Epoch 12/100, Loss: 15.610671043395996\n",
      "Epoch 13/100, Loss: 17.181842803955078\n",
      "Epoch 14/100, Loss: 31.198379516601562\n",
      "Epoch 15/100, Loss: 32.609920501708984\n",
      "Epoch 16/100, Loss: 19.893518447875977\n",
      "Epoch 17/100, Loss: 11.699503898620605\n",
      "Epoch 18/100, Loss: 16.15631866455078\n",
      "Epoch 19/100, Loss: 20.23204231262207\n",
      "Epoch 20/100, Loss: 13.777854919433594\n",
      "Epoch 21/100, Loss: 5.410059452056885\n",
      "Epoch 22/100, Loss: 5.449023246765137\n",
      "Epoch 23/100, Loss: 9.777278900146484\n",
      "Epoch 24/100, Loss: 9.06192398071289\n",
      "Epoch 25/100, Loss: 4.026313781738281\n",
      "Epoch 26/100, Loss: 2.4549527168273926\n",
      "Epoch 27/100, Loss: 5.689645290374756\n",
      "Epoch 28/100, Loss: 7.150846481323242\n",
      "Epoch 29/100, Loss: 4.367977619171143\n",
      "Epoch 30/100, Loss: 2.4242899417877197\n",
      "Epoch 31/100, Loss: 4.08329963684082\n",
      "Epoch 32/100, Loss: 5.552674293518066\n",
      "Epoch 33/100, Loss: 3.9523274898529053\n",
      "Epoch 34/100, Loss: 1.9944546222686768\n",
      "Epoch 35/100, Loss: 2.5108256340026855\n",
      "Epoch 36/100, Loss: 3.5246875286102295\n",
      "Epoch 37/100, Loss: 2.474207639694214\n",
      "Epoch 38/100, Loss: 0.9502547383308411\n",
      "Epoch 39/100, Loss: 1.1905004978179932\n",
      "Epoch 40/100, Loss: 1.9655628204345703\n",
      "Epoch 41/100, Loss: 1.4043052196502686\n",
      "Epoch 42/100, Loss: 0.49224138259887695\n",
      "Epoch 43/100, Loss: 0.7874804735183716\n",
      "Epoch 44/100, Loss: 1.3981798887252808\n",
      "Epoch 45/100, Loss: 1.0196714401245117\n",
      "Epoch 46/100, Loss: 0.48295146226882935\n",
      "Epoch 47/100, Loss: 0.7862043380737305\n",
      "Epoch 48/100, Loss: 1.1221706867218018\n",
      "Epoch 49/100, Loss: 0.7352379560470581\n",
      "Epoch 50/100, Loss: 0.3847513794898987\n",
      "Epoch 51/100, Loss: 0.6135317087173462\n",
      "Epoch 52/100, Loss: 0.7069475650787354\n",
      "Epoch 53/100, Loss: 0.3420236110687256\n",
      "Epoch 54/100, Loss: 0.19060494005680084\n",
      "Epoch 55/100, Loss: 0.39129412174224854\n",
      "Epoch 56/100, Loss: 0.36372730135917664\n",
      "Epoch 57/100, Loss: 0.12703891098499298\n",
      "Epoch 58/100, Loss: 0.1579100489616394\n",
      "Epoch 59/100, Loss: 0.3103010952472687\n",
      "Epoch 60/100, Loss: 0.21640844643115997\n",
      "Epoch 61/100, Loss: 0.10187266021966934\n",
      "Epoch 62/100, Loss: 0.2000110149383545\n",
      "Epoch 63/100, Loss: 0.2449067234992981\n",
      "Epoch 64/100, Loss: 0.1263447105884552\n",
      "Epoch 65/100, Loss: 0.10165475308895111\n",
      "Epoch 66/100, Loss: 0.1724616289138794\n",
      "Epoch 67/100, Loss: 0.12468762695789337\n",
      "Epoch 68/100, Loss: 0.043031394481658936\n",
      "Epoch 69/100, Loss: 0.07632440328598022\n",
      "Epoch 70/100, Loss: 0.09740079194307327\n",
      "Epoch 71/100, Loss: 0.037904042750597\n",
      "Epoch 72/100, Loss: 0.0316702201962471\n",
      "Epoch 73/100, Loss: 0.07388778775930405\n",
      "Epoch 74/100, Loss: 0.05339362099766731\n",
      "Epoch 75/100, Loss: 0.024900246411561966\n",
      "Epoch 76/100, Loss: 0.05214126035571098\n",
      "Epoch 77/100, Loss: 0.05725286155939102\n",
      "Epoch 78/100, Loss: 0.026398593559861183\n",
      "Epoch 79/100, Loss: 0.03040902502834797\n",
      "Epoch 80/100, Loss: 0.04301133379340172\n",
      "Epoch 81/100, Loss: 0.02102837897837162\n",
      "Epoch 82/100, Loss: 0.011994894593954086\n",
      "Epoch 83/100, Loss: 0.02558990940451622\n",
      "Epoch 84/100, Loss: 0.017297843471169472\n",
      "Epoch 85/100, Loss: 0.005416614934802055\n",
      "Epoch 86/100, Loss: 0.015785550698637962\n",
      "Epoch 87/100, Loss: 0.017154445871710777\n",
      "Epoch 88/100, Loss: 0.006915877107530832\n",
      "Epoch 89/100, Loss: 0.01168940868228674\n",
      "Epoch 90/100, Loss: 0.0159055907279253\n",
      "Epoch 91/100, Loss: 0.007577336858958006\n",
      "Epoch 92/100, Loss: 0.0074605559930205345\n",
      "Epoch 93/100, Loss: 0.011574785225093365\n",
      "Epoch 94/100, Loss: 0.005766203626990318\n",
      "Epoch 95/100, Loss: 0.003391190432012081\n",
      "Epoch 96/100, Loss: 0.006917421240359545\n",
      "Epoch 97/100, Loss: 0.003935418091714382\n",
      "Epoch 98/100, Loss: 0.0016373584512621164\n",
      "Epoch 99/100, Loss: 0.004875893704593182\n",
      "Epoch 100/100, Loss: 0.003872696543112397\n",
      "Validation Mean Squared Error: 135.58459678748602\n"
     ]
    }
   ],
   "source": [
    "# X_train represents flattened population's weights\n",
    "# y_train represents corresponding true fitness values (loss)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to('cuda')\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to('cuda')\n",
    "\n",
    "model = Surrogate(input_size=len(X_train[0]))\n",
    "model.to('cuda')\n",
    "model.load_state_dict(torch.load('surrogate_state_dict.pth'))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train surrogate model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs.view(-1), y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Validate the model on a validation set\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to('cuda')\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to('cuda')\n",
    "model.eval()\n",
    "y_val_pred = model(X_val_tensor).detach().to('cpu').numpy()\n",
    "\n",
    "# Calculate mean squared error for the validation set\n",
    "validation_mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"Validation Mean Squared Error: {validation_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fefc3df-f37d-4048-9fe5-cc5ba87606b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'sur10k_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f388ec62-be12-4fd5-885f-6c82dffdc8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_val_pred:  [161.07776]\n",
      "y_val:  153.1266237489737\n",
      "y_val_pred:  [171.26256]\n",
      "y_val:  176.8866211715018\n",
      "y_val_pred:  [174.14954]\n",
      "y_val:  175.4591851690013\n",
      "y_val_pred:  [161.98035]\n",
      "y_val:  170.74244340058345\n",
      "y_val_pred:  [193.48293]\n",
      "y_val:  181.74482211945164\n",
      "y_val_pred:  [164.69737]\n",
      "y_val:  158.6342228203063\n",
      "y_val_pred:  [130.46658]\n",
      "y_val:  127.7588722569168\n",
      "y_val_pred:  [155.00719]\n",
      "y_val:  146.35037100239165\n",
      "y_val_pred:  [151.42377]\n",
      "y_val:  158.9191265713637\n",
      "y_val_pred:  [154.96933]\n",
      "y_val:  155.6033769352421\n",
      "y_val_pred:  [138.46748]\n",
      "y_val:  132.54203626304675\n",
      "y_val_pred:  [162.77068]\n",
      "y_val:  181.40045185453573\n",
      "y_val_pred:  [150.37878]\n",
      "y_val:  158.43821201202977\n",
      "y_val_pred:  [146.34692]\n",
      "y_val:  155.8451858812077\n",
      "y_val_pred:  [161.15749]\n",
      "y_val:  173.97464212794213\n",
      "y_val_pred:  [186.79883]\n",
      "y_val:  186.30963154203573\n",
      "y_val_pred:  [158.39204]\n",
      "y_val:  164.37488201165655\n",
      "y_val_pred:  [153.41422]\n",
      "y_val:  135.56622892732074\n",
      "y_val_pred:  [182.25282]\n",
      "y_val:  185.09584900679863\n",
      "y_val_pred:  [183.0084]\n",
      "y_val:  183.88286964756668\n",
      "y_val_pred:  [155.38396]\n",
      "y_val:  152.97843281934215\n",
      "y_val_pred:  [172.7654]\n",
      "y_val:  161.04524609997014\n",
      "y_val_pred:  [158.42673]\n",
      "y_val:  145.99437135344098\n",
      "y_val_pred:  [180.44994]\n",
      "y_val:  178.76291962641818\n",
      "y_val_pred:  [162.85483]\n",
      "y_val:  165.90359205501096\n",
      "y_val_pred:  [167.75388]\n",
      "y_val:  176.54936053039162\n",
      "y_val_pred:  [145.15562]\n",
      "y_val:  165.62743936526547\n",
      "y_val_pred:  [146.20084]\n",
      "y_val:  156.10747348275154\n",
      "y_val_pred:  [184.74634]\n",
      "y_val:  178.56107578156102\n",
      "y_val_pred:  [179.74704]\n",
      "y_val:  181.53297570101014\n",
      "y_val_pred:  [184.40208]\n",
      "y_val:  179.56622732369001\n",
      "y_val_pred:  [160.10393]\n",
      "y_val:  168.21532124318895\n",
      "y_val_pred:  [170.54025]\n",
      "y_val:  162.66218450266845\n",
      "y_val_pred:  [173.7039]\n",
      "y_val:  176.33728435540655\n",
      "y_val_pred:  [180.12416]\n",
      "y_val:  210.52353809138012\n",
      "y_val_pred:  [153.94911]\n",
      "y_val:  145.6318490581148\n",
      "y_val_pred:  [170.16415]\n",
      "y_val:  169.31562513606565\n",
      "y_val_pred:  [155.92996]\n",
      "y_val:  166.25519352809638\n",
      "y_val_pred:  [168.35898]\n",
      "y_val:  169.8705497790294\n",
      "y_val_pred:  [160.09302]\n",
      "y_val:  170.7408872956683\n",
      "y_val_pred:  [157.80695]\n",
      "y_val:  160.318528971095\n",
      "y_val_pred:  [177.72488]\n",
      "y_val:  172.95602514181925\n",
      "y_val_pred:  [195.95377]\n",
      "y_val:  179.16155539348625\n",
      "y_val_pred:  [183.57289]\n",
      "y_val:  181.67380431351387\n",
      "y_val_pred:  [152.97777]\n",
      "y_val:  166.44346686685162\n",
      "y_val_pred:  [162.24182]\n",
      "y_val:  175.43140804995394\n",
      "y_val_pred:  [145.92378]\n",
      "y_val:  162.56303221101214\n",
      "y_val_pred:  [138.71745]\n",
      "y_val:  151.9081055465018\n",
      "y_val_pred:  [170.77036]\n",
      "y_val:  192.55045692784012\n",
      "y_val_pred:  [163.0519]\n",
      "y_val:  168.33471320085465\n",
      "y_val_pred:  [161.93182]\n",
      "y_val:  148.0027252002886\n",
      "y_val_pred:  [179.3212]\n",
      "y_val:  165.89649263612785\n",
      "y_val_pred:  [176.54637]\n",
      "y_val:  155.6734678426366\n",
      "y_val_pred:  [164.60806]\n",
      "y_val:  152.8923232935037\n",
      "y_val_pred:  [189.43246]\n",
      "y_val:  178.5944450038254\n",
      "y_val_pred:  [165.49458]\n",
      "y_val:  167.6465146010089\n",
      "y_val_pred:  [127.93378]\n",
      "y_val:  118.1850771569902\n",
      "y_val_pred:  [178.86931]\n",
      "y_val:  170.06730690731365\n",
      "y_val_pred:  [169.69989]\n",
      "y_val:  202.87203853145527\n",
      "y_val_pred:  [150.13042]\n",
      "y_val:  150.07468681578425\n",
      "y_val_pred:  [133.36516]\n",
      "y_val:  158.51287627979448\n",
      "y_val_pred:  [157.50096]\n",
      "y_val:  153.2173656293541\n",
      "y_val_pred:  [165.12363]\n",
      "y_val:  156.08372176832455\n",
      "y_val_pred:  [153.59146]\n",
      "y_val:  192.68792520510922\n",
      "y_val_pred:  [173.26485]\n",
      "y_val:  188.19472722339023\n",
      "y_val_pred:  [155.55127]\n",
      "y_val:  170.4286368789187\n",
      "y_val_pred:  [193.38023]\n",
      "y_val:  183.71436936688272\n",
      "y_val_pred:  [182.45648]\n",
      "y_val:  178.44525904564344\n",
      "y_val_pred:  [155.63182]\n",
      "y_val:  162.81539411605544\n",
      "y_val_pred:  [163.2033]\n",
      "y_val:  179.73253296287197\n",
      "y_val_pred:  [161.77808]\n",
      "y_val:  167.07765984990795\n",
      "y_val_pred:  [172.74721]\n",
      "y_val:  179.67271559405478\n",
      "y_val_pred:  [160.66142]\n",
      "y_val:  169.49265265009208\n",
      "y_val_pred:  [173.08638]\n",
      "y_val:  166.0396255201595\n",
      "y_val_pred:  [157.37614]\n",
      "y_val:  162.87460842254055\n",
      "y_val_pred:  [175.445]\n",
      "y_val:  161.5094616154956\n",
      "y_val_pred:  [161.11432]\n",
      "y_val:  156.13912244663118\n",
      "y_val_pred:  [151.7489]\n",
      "y_val:  144.96831711690137\n",
      "y_val_pred:  [178.25119]\n",
      "y_val:  186.03804813069144\n",
      "y_val_pred:  [141.58038]\n",
      "y_val:  130.2104303639406\n",
      "y_val_pred:  [164.77974]\n",
      "y_val:  171.5968881594907\n",
      "y_val_pred:  [155.00307]\n",
      "y_val:  150.50443175492012\n",
      "y_val_pred:  [171.56409]\n",
      "y_val:  166.12365100641918\n",
      "y_val_pred:  [151.50838]\n",
      "y_val:  165.05118419865894\n",
      "y_val_pred:  [138.72984]\n",
      "y_val:  145.3696867827397\n",
      "y_val_pred:  [188.95271]\n",
      "y_val:  176.7332976517404\n",
      "y_val_pred:  [137.08139]\n",
      "y_val:  139.050500954792\n",
      "y_val_pred:  [156.42122]\n",
      "y_val:  153.73331417399606\n",
      "y_val_pred:  [204.99583]\n",
      "y_val:  191.02476627811504\n",
      "y_val_pred:  [150.03302]\n",
      "y_val:  145.7998302969963\n",
      "y_val_pred:  [168.61833]\n",
      "y_val:  175.85460910675633\n",
      "y_val_pred:  [170.58386]\n",
      "y_val:  171.22117031607655\n",
      "y_val_pred:  [188.62846]\n",
      "y_val:  182.9213208241068\n",
      "y_val_pred:  [166.94026]\n",
      "y_val:  165.30284322750796\n",
      "y_val_pred:  [161.06007]\n",
      "y_val:  179.8962546184564\n",
      "y_val_pred:  [174.24733]\n",
      "y_val:  189.1595689324057\n",
      "y_val_pred:  [158.96431]\n",
      "y_val:  175.64523373743535\n",
      "y_val_pred:  [153.55202]\n",
      "y_val:  154.21257504991664\n",
      "y_val_pred:  [192.46054]\n",
      "y_val:  176.3606276785492\n",
      "y_val_pred:  [161.08075]\n",
      "y_val:  156.08292218833972\n",
      "y_val_pred:  [189.70444]\n",
      "y_val:  216.51153370073646\n",
      "y_val_pred:  [181.3323]\n",
      "y_val:  172.42941128676105\n",
      "y_val_pred:  [159.94052]\n",
      "y_val:  161.07683470902168\n",
      "y_val_pred:  [162.86201]\n",
      "y_val:  170.74395227128534\n",
      "y_val_pred:  [163.3712]\n",
      "y_val:  147.66893802326956\n",
      "y_val_pred:  [169.19666]\n",
      "y_val:  177.70694494551154\n",
      "y_val_pred:  [154.84502]\n",
      "y_val:  167.3799865139518\n",
      "y_val_pred:  [151.57867]\n",
      "y_val:  141.16666980913487\n",
      "y_val_pred:  [152.30779]\n",
      "y_val:  160.00814790179012\n",
      "y_val_pred:  [155.02032]\n",
      "y_val:  159.4224794229884\n",
      "y_val_pred:  [186.06859]\n",
      "y_val:  182.49747413586658\n",
      "y_val_pred:  [167.2748]\n",
      "y_val:  168.50415175128134\n",
      "y_val_pred:  [178.20868]\n",
      "y_val:  187.3829796663515\n",
      "y_val_pred:  [171.8638]\n",
      "y_val:  166.3999799011619\n",
      "y_val_pred:  [147.31494]\n",
      "y_val:  135.09247783490807\n",
      "y_val_pred:  [154.95592]\n",
      "y_val:  165.51565551757812\n",
      "y_val_pred:  [154.40115]\n",
      "y_val:  164.466227343128\n",
      "y_val_pred:  [167.0388]\n",
      "y_val:  171.70580024476263\n",
      "y_val_pred:  [135.6816]\n",
      "y_val:  133.476257227029\n",
      "y_val_pred:  [158.70416]\n",
      "y_val:  152.65883038757713\n",
      "y_val_pred:  [171.43327]\n",
      "y_val:  163.83099520737957\n",
      "y_val_pred:  [165.13979]\n",
      "y_val:  168.47535647252562\n",
      "y_val_pred:  [182.6147]\n",
      "y_val:  195.41874111685783\n",
      "y_val_pred:  [155.87892]\n",
      "y_val:  171.81680997617684\n",
      "y_val_pred:  [180.1779]\n",
      "y_val:  180.58741974071333\n",
      "y_val_pred:  [153.1023]\n",
      "y_val:  155.4134708574623\n",
      "y_val_pred:  [179.31204]\n",
      "y_val:  152.763228689789\n",
      "y_val_pred:  [150.39412]\n",
      "y_val:  155.35877524211907\n",
      "y_val_pred:  [150.64821]\n",
      "y_val:  160.68832416898886\n",
      "y_val_pred:  [153.06937]\n",
      "y_val:  145.1983216887067\n",
      "y_val_pred:  [168.28879]\n",
      "y_val:  170.3413791535007\n",
      "y_val_pred:  [154.75084]\n",
      "y_val:  169.31227150692303\n",
      "y_val_pred:  [158.74977]\n",
      "y_val:  141.19388769842257\n",
      "y_val_pred:  [164.00497]\n",
      "y_val:  184.40800563544985\n",
      "y_val_pred:  [141.78699]\n",
      "y_val:  135.42504348268935\n",
      "y_val_pred:  [174.27405]\n",
      "y_val:  156.28973932934414\n",
      "y_val_pred:  [198.88437]\n",
      "y_val:  203.1578931869215\n",
      "y_val_pred:  [163.62978]\n",
      "y_val:  162.80011734689117\n",
      "y_val_pred:  [172.79823]\n",
      "y_val:  174.81667488243926\n",
      "y_val_pred:  [139.50708]\n",
      "y_val:  159.6133796060161\n",
      "y_val_pred:  [184.91566]\n",
      "y_val:  216.00801931976514\n",
      "y_val_pred:  [129.3751]\n",
      "y_val:  140.5289373701545\n",
      "y_val_pred:  [159.36356]\n",
      "y_val:  161.61376476895276\n",
      "y_val_pred:  [176.27728]\n",
      "y_val:  168.84912332911398\n",
      "y_val_pred:  [166.62514]\n",
      "y_val:  161.01068309613854\n",
      "y_val_pred:  [195.689]\n",
      "y_val:  203.91431878934242\n",
      "y_val_pred:  [169.21912]\n",
      "y_val:  163.84524098778985\n",
      "y_val_pred:  [196.6928]\n",
      "y_val:  181.47184889483603\n",
      "y_val_pred:  [174.6862]\n",
      "y_val:  160.25658790928543\n",
      "y_val_pred:  [177.77576]\n",
      "y_val:  174.138030033962\n",
      "y_val_pred:  [173.68518]\n",
      "y_val:  193.9628603989911\n",
      "y_val_pred:  [144.83946]\n",
      "y_val:  155.32731074436455\n",
      "y_val_pred:  [162.19534]\n",
      "y_val:  176.83270409456483\n",
      "y_val_pred:  [140.87524]\n",
      "y_val:  158.39621039712506\n",
      "y_val_pred:  [150.80106]\n",
      "y_val:  154.58252827832652\n",
      "y_val_pred:  [143.19617]\n",
      "y_val:  144.81466004195488\n",
      "y_val_pred:  [183.66895]\n",
      "y_val:  184.74798341010026\n",
      "y_val_pred:  [146.38107]\n",
      "y_val:  162.13915957311156\n",
      "y_val_pred:  [146.57]\n",
      "y_val:  145.09193609930148\n",
      "y_val_pred:  [177.5566]\n",
      "y_val:  192.21729574993157\n",
      "y_val_pred:  [171.24364]\n",
      "y_val:  155.82406985531946\n",
      "y_val_pred:  [156.99617]\n",
      "y_val:  157.987422092705\n",
      "y_val_pred:  [187.4886]\n",
      "y_val:  175.5666835323261\n",
      "y_val_pred:  [153.01053]\n",
      "y_val:  152.3531055814901\n",
      "y_val_pred:  [145.35379]\n",
      "y_val:  153.29714547904433\n",
      "y_val_pred:  [160.43677]\n",
      "y_val:  163.58746726649582\n",
      "y_val_pred:  [162.09668]\n",
      "y_val:  148.42543690675384\n",
      "y_val_pred:  [140.69804]\n",
      "y_val:  162.45489385325442\n",
      "y_val_pred:  [159.04175]\n",
      "y_val:  164.22409942043814\n",
      "y_val_pred:  [140.8842]\n",
      "y_val:  140.3563558979399\n",
      "y_val_pred:  [174.45274]\n",
      "y_val:  162.37191918245546\n",
      "y_val_pred:  [176.2212]\n",
      "y_val:  176.86567007660108\n",
      "y_val_pred:  [178.869]\n",
      "y_val:  173.3971407944989\n",
      "y_val_pred:  [156.93173]\n",
      "y_val:  167.69095801092257\n",
      "y_val_pred:  [167.98584]\n",
      "y_val:  154.6987842146758\n",
      "y_val_pred:  [150.72166]\n",
      "y_val:  152.14967628041651\n",
      "y_val_pred:  [181.49171]\n",
      "y_val:  174.90166118646124\n",
      "y_val_pred:  [146.95609]\n",
      "y_val:  167.2637625530267\n",
      "y_val_pred:  [165.73772]\n",
      "y_val:  184.70750378651223\n",
      "y_val_pred:  [163.1571]\n",
      "y_val:  183.53553820567527\n",
      "y_val_pred:  [188.1908]\n",
      "y_val:  170.3860108199393\n",
      "y_val_pred:  [160.59]\n",
      "y_val:  157.0966539322191\n",
      "y_val_pred:  [171.23743]\n",
      "y_val:  177.94381159885674\n",
      "y_val_pred:  [162.96144]\n",
      "y_val:  176.76493932638957\n",
      "y_val_pred:  [156.80627]\n",
      "y_val:  167.95299496012885\n",
      "y_val_pred:  [155.38736]\n",
      "y_val:  174.33742994563596\n",
      "y_val_pred:  [170.42564]\n",
      "y_val:  174.62223193903637\n",
      "y_val_pred:  [158.7952]\n",
      "y_val:  162.00446023151375\n",
      "y_val_pred:  [151.37749]\n",
      "y_val:  171.36852104041228\n",
      "y_val_pred:  [184.60051]\n",
      "y_val:  173.22248976397665\n",
      "y_val_pred:  [138.14598]\n",
      "y_val:  144.74318753382204\n",
      "y_val_pred:  [164.88031]\n",
      "y_val:  170.98984697669934\n",
      "y_val_pred:  [162.54907]\n",
      "y_val:  172.768452929843\n",
      "y_val_pred:  [183.3553]\n",
      "y_val:  164.98176147072178\n",
      "y_val_pred:  [173.588]\n",
      "y_val:  165.48215800485792\n",
      "y_val_pred:  [147.30109]\n",
      "y_val:  148.01545705613057\n",
      "y_val_pred:  [150.97125]\n",
      "y_val:  154.23537032011967\n",
      "y_val_pred:  [166.24075]\n",
      "y_val:  167.8124200128446\n",
      "y_val_pred:  [165.33092]\n",
      "y_val:  162.99818391253234\n",
      "y_val_pred:  [176.16551]\n",
      "y_val:  172.1147926476351\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]))\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    print(\"y_val_pred: \", y_val_pred[i])\n",
    "    print(\"y_val: \", y_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff0b15-7a8a-4aba-a8d3-26b75edc4e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
